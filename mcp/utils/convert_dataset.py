import numpy as np
import lmdb
import msgpack
import zlib
import sqlite3
import pandas as pd
import joblib
from joblib import Parallel, delayed

MAX_THREADS = 4
MAX_RECORDS_PER_FILE = 25000


def features_postproc_func(x):
    x = np.asarray(x[0], dtype=np.float32)
    return x


def create_csv(file, offset, rows):
    df = pd.DataFrame(rows, columns=['sha', 'is_malware', 'rl_fs_t', 'rl_ls_const_positives', 'adware', 'flooder', 'ransonware', 'dropper', 'spyware', 'packed',
                                     'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader'] + [f'feature_{x}' for x in range(2381)])
    rows.clear()
    print(f'Creating csv file for file {file} offset {offset}....')
    df.to_csv(f'sorel_{file:04d}_p{offset:03d}.csv.gz',
              index=False, header=True, compression='gzip')


def convert_dataset(mod, offset):
    lmdb_file = "ember_features"
    lmdb_env = lmdb.open(lmdb_file)
    lmdb_txn = lmdb_env.begin()
    lmdb_cursor = lmdb_txn.cursor()

    sqlite_conn = sqlite3.connect('meta.db')
    query = "select * from meta where sha256 = '%s'"
    sqlite_cur = sqlite_conn.cursor()

    index = 0
    file = 0
    rows = []
    print('started......')
    for key, value in lmdb_cursor:
        index += 1
        if index % mod != offset:
            continue
        features = features_postproc_func(msgpack.loads(
            zlib.decompress(value), strict_map_key=False))

        val = sqlite_cur.execute(query % key.decode('utf-8')).fetchone()
        if val is None:
            continue

        sha, is_malware, rl_fs_t, rl_ls_const_positives, adware, flooder,\
            ransonware, dropper, spyware, packed, crypto_miner, file_infector,\
            installer, worm, downloader = val

        rows.append([sha, int(is_malware == 1), float(rl_fs_t),\
            int(rl_ls_const_positives), int(adware), int(flooder), int(ransonware), int(dropper), int(spyware), int(
            packed), int(crypto_miner), int(file_infector), int(installer), int(worm),  int(downloader)] + list(features))
        if len(rows) >= MAX_RECORDS_PER_FILE:
            create_csv(file, offset, rows)
            file += 1

    create_csv(file, offset, rows)
    sqlite_conn.close()
    print('finished......')


if __name__ == '__main__':
    number_of_cpu = MAX_THREADS
    delayed_funcs = [delayed(convert_dataset)(
        number_of_cpu, x) for x in range(number_of_cpu)]

    parallel_pool = Parallel(n_jobs=number_of_cpu)
    parallel_pool(delayed_funcs)
